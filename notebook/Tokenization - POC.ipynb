{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content pre-processing step takes in both normal text and code, performs \n",
    "* Tokenization: Tokenization breaks a paragraph into word tokens.\n",
    "* Stop word removal: Stop word removal removes commonly used words like: is, are, I, you, etc.\n",
    "* Stemming: Stemming reduces a word to its root form, e.g., reading to read, etc. \n",
    "\n",
    "For the code, we remove reserved keywords such as: if, while, etc.,\n",
    "curly brackets, etc, and extract identifiers and comments. These are then subjected to tokenization, stemming, and\n",
    "stop word removal too [1].\n",
    "\n",
    "Additional preprocessing steps of code snippets we hope to use influenced from [2] and [3]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "1. Wang et. al.\n",
    "2. Indentifier tokenization: Son Nguyen, Hung Phan, Trinh Le, and Tien N. Nguyen. 2020. Suggesting natural method names to check name consistencies. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering (ICSE '20). Association for Computing Machinery, New York, NY, USA, 1372â€“1384. DOI:https://doi.org/10.1145/3377811.3380926\n",
    "3. https://stackoverflow.com/questions/29916065/how-to-do-camelcase-split-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nimmi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"How to escape - (hyphen) using the groovy language\"\n",
    "text = \"I am trying to declare a variable that requires hyphen as part of the design spec. However, i am getting this error - https://www.tutorialspoint.com/execute_groovy_online.php\"\n",
    "codes = [\n",
    "    \"def user-svc = \\\"accounts\\\"\",\n",
    "    \"$groovy main.groovy \\n Hello world \\n Caught: groovy.lang.MissingPropertyException: No such property: user for class: main \\n groovy.lang.MissingPropertyException: No such property: user for class: main \\n at main.run(main.groovy:3)\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important:\n",
    "* title could be a error message\n",
    "* text could contain links\n",
    "* code could be invalid, For example, code could contain runtime errors copies pasted from the console"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tockenize(text):\n",
    "    tokens = text.split(\" \")\n",
    "    if \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'to', 'escape', '-', '(hyphen)', 'using', 'the', 'groovy', 'language']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tockenize(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def', 'user-svc', '=', '\"accounts\"']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tockenize(codes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Remove non-alphabetic tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_alphabetic_tokens(tokens):\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'to', 'escape', 'using', 'the', 'groovy', 'language']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tockenize(title)\n",
    "remove_non_alphabetic_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['def']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tockenize(codes[0])\n",
    "remove_non_alphabetic_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing non alphabetic tokens is not working for codes. Codes should be preprocessed before applying these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'escape', 'using', 'groovy', 'language']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tockenize(title)\n",
    "tokens = remove_non_alphabetic_tokens(tokens)\n",
    "remove_stop_words(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(tokens):\n",
    "    ps = PorterStemmer() \n",
    "    tokens = [ps.stem(w) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'to', 'escap', 'use', 'the', 'groovi', 'languag']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tockenize(title)\n",
    "tokens = remove_non_alphabetic_tokens(tokens)\n",
    "tokens = remove_stop_words(tokens)\n",
    "stem(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: remove short tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_short_tokens(tokens,length=1):\n",
    "    tokens = [word for word in tokens if len(word) > length]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How', 'escape', 'using', 'groovy', 'language']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tockenize(title)\n",
    "tokens = remove_non_alphabetic_tokens(tokens)\n",
    "tokens = remove_stop_words(tokens)\n",
    "remove_short_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code preprocess before tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"For the code, we remove reserved keywords such as: if, while, etc., curly brackets, etc, and extract identifiers and comments. These are then subjected to tokenization, stemming, and stop word removal too.\" [1].\n",
    "\n",
    "What we planned to do:\n",
    "1. Extract Identifiers\n",
    "2. Extract Comments\n",
    "3. Tokenization, stemming, and stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
